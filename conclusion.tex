\section{Conclusion}

In this paper we presented a series of loop transformations and showed
how they behave on CPUs and GPUs when applied to a computational steering
application. We argued that giving the architectural similitude of CPUs and GPUs
seen both as vector processors with different vector units sizes and roughly
same cache hierarchy, some optimizations, at least conceptually, should be the
same. With these concepts in mind, we proposed an extensive set of optimizations
for both CPUs and GPUs that provide a speedup of up to 12.8x and 5.2x
respectively.

In addition, we presented the first implementation of hierarchization and
interpolation algorithms for adaptive sparse grids on GPUs and proved that our
data structure and associated algorithms are easily parallelizable using
architecture independent loop transformations.

We see the convergence of CPUs and GPUs both in terms of hardware features and
programming languages. On heterogeneous systems OpenCL \cite{opencl} is slowly
making its way, but it is not as mature as CUDA and remains similarly complex.
OpenACC \cite{openacc} is a new standard which has a lot of promise focusing on
parallelization of loop nests. Using common transformations and with the help of
autotuning techniques for evaluation, the differences in programming CPUs and
GPUs will be in the future hidden inside these kinds of frameworks and
libraries.

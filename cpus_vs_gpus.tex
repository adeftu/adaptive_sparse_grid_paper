\section{CPUs and GPUs}
\label{sec:cpus_vs_gpus}
Heterogeneous systems containing CPUs and accelerators allow us to reach higher
computational speeds while keeping power consumption at acceptable levels. The
latest developments show a trend towards an unification in terms of hardware
features of both architectures. Instead of focusing only on the classic way to
increase the speed of CPUs by raising the frequency with each new release, the
manufacturers are now trying to exploit other directions as well, such as
vectorization or many-cores. An eloquent example is Intel's Many Integrated
Core Architecture (MIC) \cite{intel_mic} which is announced to include 512-bit
wide vector registers and more than 50 cores per chip (in Knights Corner product
line). On the other hand, GPUs manufacturers started to include CPU-like
features in their products. Fermi \cite{fermi} was the first GPU generation from
Nvidia featuring a cache coherent memory and today all devices tend to have a
cache hierarchy similar to the one on CPUs.

\subsection{GPU Architecture}

GPUs are heavily optimized for fast arithmetic operations and in consequence
have many dedicated floating point units. Therefore, not all applications are
suitable for running on GPUs, or at least not as efficient as on CPUs, but only
those that exhibit regular memory access pattern and require a great amount of
computational power.

The CUDA architecture from Nvidia is based on \textit{Streaming Multiprocessors}
(SM), which are SIMD cores with the role of creating, scheduling and executing
concurrent threads. One such multiprocessor is composed of several lanes, or
\textit{Scalar Processors} (SP), and has a dedicated shared memory for fast
access by the threads running on it. The thread-execution manager automatically
schedules threads on processors without the need from the programmer for writing
explicit code. The execution is done in groups of 32 threads, called
\textit{warps}, such that every instruction is broadcast synchronously to all
the active threads in a \textit{warp}. The degree of parallelism is high when
all these 32 threads execute the same instruction and very low when branching
causes thread divergence.

Each thread has a small local private memory. Threads are grouped in
\textit{blocks}. Each \textit{block} has a shared memory accessible by all
threads in the \textit{block}. Designed for graphics processing, the cards are
also equipped with dedicated, read-only, on-chip memories: constant cache for
low latency and texture cache for high bandwidth. Finally, there is also the
global memory which has the largest capacity, but is the slowest. Starting with
the Fermi architecture, a finer level of control is possible for configuring how
the allocation of these memories is done, giving the programmer more
flexibility, but at the cost of exposing complexities of the underlying
architecture.

\subsection{GPU Programming}

While SIMD code on CPUs can be written using assembly, a high-level language
like C (with intrinsic functions), libraries such as Intel Array Building Blocks
or just relying on a compiler with auto-vectorizing capabilities, Nvidia GPUs
are usually  programmed through their dedicated framework, called CUDA. CUDA
allows the programmer to define functions, or \textit{kernels}, to be executed
in parallel by many threads on different processors. When the \textit{kernels}
are launched on the GPU, a given topology must be specified. Threads are grouped
in \textit{thread blocks} which are laid in a grid. Each \textit{thread block}
has a unique 3d index which gives its position in the grid, while each thread
has a 3d index identifying the thread in a \textit{block}. This allows uni-,
bi-, and three-dimensional topologies, the right configuration depending on the
problem to solve. Threads in a \textit{block} are scheduled to be run on a
single SM. This means synchronization is only possible among threads in the same
\textit{block}, using the \emph{\_\_syncthreads} barrier. These threads can also
share data through the shared memory of the SM.

Optimizations on GPUs address mostly thread scheduling and memory access. The
most important ones in the first category are maximizing the number of active
threads per SM by choosing the right \textit{kernel} topology and minimizing the
number of divergent branches in a \textit{warp} that could lead to a serial
execution. Regarding the memory hierarchy, it is preferable to use on-chip
memories whenever possible, to reduce the number of bank conflicts when
accessing the shared memory, and to reduce the number of memory transactions to
main memory through coalescing. For a detailed list of optimizations we refer
the reader to \cite{cuda}.

\section{Computational Steering Using Sparse Grids}
\label{sec:comp_steering}

Our application is the visualization of compressed, high-dimensional data
resulting from simulations \cite{Butnaru201156}. It is a well known fact that
managing the data coming from functions with high number of dimensions has an
exponential complexity on the number of dimensions. Therefore, we compress the
data using the sparse grid technique in order to reduce its size and we
decompress it afterwards for real-time visualization. For this purpose we use
the \textit{fastsg} library presented in \cite{murarasu12fastsg:}.

The main idea behind sparse grids is that we can approximate a $D$-dimensional
function $f : \Omega \rightarrow \mathbb{R}$, where $\Omega = [0, 1]^{D}$, by
discretizing $\Omega$ and representing $f$ as a weighted sum of basis functions.
If we consider \textit{levels} $\bar{l} = (l_{1},\ldots,l_{D})$ and
\textit{index} $\bar{i} = (i_{1},\ldots,i_{D})$ vectors in $\mathbb{N}^{D}$,
then each basis function will be centered at points $\bar{x}_{\bar{l},\bar{i}} =
(x_{l_{1},i_{1}},\ldots,x_{l_{D},i_{D}}) \in \mathbb{Q}^{D}$, for which $x_{l,i}
= i \cdot 2^{-l}$, $i \in \{1,\ldots,2^{l} - 1\}$, and $i$ odd. This means that
each grid point $\bar{x}_{\bar{l},\bar{i}}$ can be uniquely identified by the
pair $(\bar{l},\bar{i})$. Thus,

\[ f \approx \sum_{\bar{x}_{\bar{l},\bar{i}}} \alpha_{\bar{l},\bar{i}} \cdot
\phi_{\bar{l},\bar{i}} \]
where $\alpha_{\bar{l},\bar{i}}$ is the weight, or hierarchical coefficient, and
$\phi_{\bar{l},\bar{i}}$ is the basis function centered at grid point
$\bar{x}_{\bar{l},\bar{i}}$. In our case, $\phi_{\bar{l},\bar{i}}$ is obtained
by multiplying $D$ one-dimensional functions $\phi_{l,i} = h(2^{l}x - i)$, where
$h$ is the standard hat function $h(x) = max(1 - |x|, 0)$:

\[ \phi_{\bar{l},\bar{i}}(\bar{x}) = \prod_{t=1}^{D} \phi_{l,i}(x_{t}) .\]

Compressing a general function represented on a full grid is done by selecting
only the function values at grid points also contained in a sparse grid. This is
reduced to computing the hierarchical coefficients, process called
\textit{hierarchization}. The method is somehow related to wavelet
transformation technique, where the computing and grouping the coefficients
leads to different levels of compression and thus an incremental representation.
Decompression (or \textit{interpolation}) refers to evaluating the function
anywhere inside the domain by summing up the contributions of the basis
functions averaged by their hierarchical coefficients. This technique also
enables us to interpolate at points for which we do not have values from
simulation. Hence, it can provide hints on the simulation outside the initial
data. In our case, decompression is a form of interpolation based on the sparse
grid technique described in \cite{CambridgeJournals:227245}. To allow a fluent
interaction with data, interpolation has to run as fast as possible. This is
equivalent to ensuring that no processor in a heterogeneous system is idle at
any moment of time and that each processor is used at its full potential.

\begin{figure}[t]
  \begin{subfigure}[b]{1\linewidth}
    \centering
    \includegraphics[width=0.8\textwidth]{truncated_sparse_grid_deco_1.pdf}
    \caption{Regular sparse grid.}
  \end{subfigure}
  \\ \\
  \begin{subfigure}[b]{1\linewidth}
    \centering
    \includegraphics[width=0.8\textwidth]{truncated_sparse_grid_deco_2.pdf}
    \caption{Dimensionally truncated sparse grid.}
  \end{subfigure}
  \caption{Decomposition of regular and dimensionally truncated 2D sparse
  grids.}
  \label{fig:truncated_sparse_grid_deco}
\end{figure}

It is important that sparse grids should not be confused with sparse matrices.
They differ in both functionality and computational behavior. As mentioned,
sparse grids allow us to approximate multi-dimensional functions. Instead of
using the traditional approach of storing them in trees and hash-tables, we are
using bijection based data structure with minimum memory footprint
\cite{Murarasu:2011:CDS:1941553.1941559}, which maps all $(\bar{l},\bar{i})$
pairs to consecutive integer indices. Thus, all hierarchical coefficients of the
grid will be stored efficiently in a 1-dimensional array.

Fig.~\ref{fig:truncated_sparse_grid_deco} shows that a sparse grid can be
represented as a sequence of regular grids, grouped by their norm into several
refinement levels, which we'll call them \textit{groups} from now on. The grids
in one \textit{group} will be referred as \textit{blocks}.
Interpolating at a given $D$-dimensional point means traversing the set of
regular grids and computing the contribution of each regular grid on the result. For
each regular grid a $D$-linear basis function ($\Otime{D}$) is built and
evaluated at the point. Interpolating at one point uses exactly one value from each
regular grid for scaling the basis function. Using this knowledge, we can now
look at the influence of the inputs on performance. All of these parameters
influence to some extent the performance behavior of interpolation, especially
on the GPU.

Sparse grid interpolation has 5 input parameters: the number of dimensions
($D$), the refinement level ($L$), the number of interpolations ($N$), the
precision ($P$) (single or double precision), and the truncation ($T$)
(dimensionally truncated or regular). In this paper we concentrate on the first
3, these being the most important as they can take a wide range of values.

$D$ increases the computational intensity, i.e. the ratio between the on-chip
computation time and off-chip communication time. On GPU, a large $D$ causes an
increased consumption of shared memory per thread reducing the benefits of
multithreading. A large $L$ decreases the computational intensity since the size
of the regular grids increases exponentially, i.e. from $2^0$ to $2^{L-1}$. 

As only one regular grid value is used per interpolation, only a small
percentage of the compressed data transferred over PCIe to the GPU is actually
used for computation. $N$ is proportional to the computational intensity, i.e
the more interpolations we perform, the more worthwhile is the data transfer over
PCIe.

The dimensional truncation parameter gives the possibility of reducing the
computational effort when using functions where not all input variables carry
equal weight. Therefore, instead of storing all regular grids, we can exclude
some of them when the information on those dimensions is not critical.

Our versions of interpolation are based on the iterative algorithm from
\cite{Murarasu:2011:CDS:1941553.1941559} over which we applied a series of
optimizations for both CPUs and GPUs. Having these two versions of
interpolation, we combine them so that all the processors in a heterogeneous
system simultaneously work on interpolation. In general, on the systems where we
measured the performance of interpolation, the GPU was faster than the CPU. But,
since our goal is performance portability, it makes sense to consider the
situation in which the GPU is not faster than the multi-core CPUs available in
the system. This can be the case for instance with Intel's Sandy Bridge
processors which have a SIMD unit~\cite{avx} (256~bit AVX) twice as wide as the
previous generation, Nehalem (128~bit SSE).
\section{Optimization Strategies}
\label{sec:op_strategies}

The computational core of \textit{fastsg} resides in \textit{hierarchize} and
\textit{evaluate} routines. We will not enter here into the details of these
procedures. A summary of them is available in
\cite{Murarasu:2011:CDS:1941553.1941559}.
Instead, we extend the set of loop transformations from \cite{murarasu12fastsg:} and
present them in the context of CPUs and GPUs. We group these optimizations into
3 categories based on their end purpose: memory locality, vectorization, and
operations with integers, synthesized in Table \ref{table:optimizations}.

We start with a basic version of \textit{hierarchization} algorithm for GPU
where we loop with $t$ variable over all dimensions, with $g$ variable over all
\textit{groups} and then we call the kernel procedure. Each warp on the GPU
computes the coefficients for one \textit{block} of the sparse grid in order to
achieve a better cache locality and to reduce the thread divergence within
warps. Because the coefficients for one \textit{block} are stored contiguously
in memory, we also want for these values to be accessed simultaneously.
Therefore, each thread within a warp starts reading at index equal to its lane
and then subsequently every element found at a distance multiple of 32 (the
number of threads within a warp). In this way, a warp will access the memory
contiguously at each read instruction with an increased throughput. Here $l$ and
$i$ are the \textit{levels} and respectively \textit{index} vectors which
represent the coordinates of one point in the grid. We store them in the shared
memory because they are frequently used. The routines \textit{left} and
\textit{right} return the left and respectively right parents (\textit{blocks}
that the current one depends on) of a point in dimension $t$, while
\textit{agp2idx} returns the index where point $(l, i)$ is stored in the 1d
representation of the sparse grid, $asg1d$.

\begin{algorithm}[b]
\small{
	\caption{Hierarchization on GPU}
 	\label{alg:hierarchization}                       

 	\begin{algorithmic}[1]
 		\For{$t=1$ to $D$}
 			\For{$g=L$ downto $1$}
 				\State \Call{HierarchizeKernel}{$t, g$}
 			\EndFor
 		\EndFor
 		\algstore{bkbreak}
	\end{algorithmic}
 
	\begin{algorithmic}[1]
  		\algrestore{bkbreak}
  		\Procedure{HierarchizeKernel}{$t, g$}
	 		\For{$j \leftarrow \Call{BlockStart}{g} + lane;$ \\
	 		    \hspace{24pt} $j < \Call{BlockEnd}{g}; \; j\leftarrow j + 32$}
	 			\If{$lane = 0$}
	 				\State $l \leftarrow idx2l(j)$
	 			\EndIf
	 			\State $i \leftarrow idx2i(j)$
	 			\State $(\textit{ll}, \textit{il}) \leftarrow \text{left}(l, i, t)$
	 			\State $(\textit{lr}, \textit{ir}) \leftarrow \text{right}(l, i, t)$
	 			\State $\textit{v1} \leftarrow \textit{asg1d}[\text{agp2idx}(\textit{ll},
	 			\textit{il})]$ \State $\textit{v2} \leftarrow
	 			\textit{asg1d}[\text{agp2idx}(\textit{lr}, \textit{ir})]$ \State$\textit{asg1d}[j] \leftarrow \textit{asg1d}[j] - (\textit{v1} +\textit{v2}) \cdot 0.5$
			\EndFor
		\EndProcedure
	\end{algorithmic}
}
\end{algorithm}

We now improve this basic algorithm and we start with optimizations for integer
operations. The first observation we make is that computing the $l$ vector for
each point in the \textit{block} is redundant because within a \textit{block},
it has the same value. Therefore loop invariant code motion optimization
\textit{inv1} moves the computation of $l$ outside the $j$ loop. 

Next, in \textit{inv2} we can compute for each \textit{block} of norm $g$ the
indices in $asg1d$ of all $g-1$ parent \textit{blocks} before the innermost
loop. We store them in an array (in the heap space in the case of CPU
implementation and in the shared memory of a CUDA multiprocessor for the GPU
version). To get the values for these parent points, we can now use this array
to index $asg1d$ and get the start of the \textit{block}. What we still need is
the index inside the \textit{block}. This is computed inside the $j$ loop by
converting $i$ to the corresponding index.

\textit{inv3} starts from the observation that $(ll, il)$ and $(lr, ir)$, the
\textit{levels} and \textit{index} vectors of the left and right parents, differ
from $l$ and $i$ of the current point only in one dimension. Therefore, the
conversion mentioned above is not necessary anymore, thus reducing the
complexity from $O(D)$ to $O(1)$.

Now, in the innermost loop over the current \textit{block}, what is left to
compute is the conversion $(l(t), i(t)) \rightarrow (ll(t), il(t))$ and $(l(t),
i(t)) \rightarrow (lr(t), ir(t))$ which has a complexity of $O(l(t))$.
\textit{inv4} precomputes the results of these conversions before the kernel
launch and stores them in an array. The advantage is that the complexity of the
innermost loop is now $O(1)$, but at a cost of higher memory footprint.

Lastly, in order to improve memory locality and the data access patterns,
\textit{ichg1} optimization does a loop interchange by moving the loop over the
\textit{groups} from line 2 inside the kernel. In this way, one warp will not
only process one \textit{block}, but also all its parents.

We move now to the algorithm for \textit{evaluation}. The coordinates of all $N$
points to be evaluated are stored contiguously in a matrix $x[N][D]$ which is
processed in chunks by each warp, each warp computing one chunk of size $w$. The
memory access pattern for one thread inside of its chunk is the same as for
\textit{hierarchization} algorithm where each thread computes points stored at
indices multiple of 32 starting from the corresponding lane. The $g$ and $b$
loops traverse \textit{groups} and \textit{blocks} respectively.
$\textit{idx23}$ is used to index the beginning of the current \textit{block} in
$\textit{asg1d}$, while $idx1$ idexes the current point in this \textit{block}.

\begin{algorithm}[b]
\small{
	\caption{Evaluation on GPU}
	\label{alg:evaluation}
	\begin{algorithmic}[1]
 		\Procedure{EvaluateKernel}{$w$}
    		\For{$j \leftarrow \Call{ChunkStart}{w} + lane;$ \\ 
    		\hspace{24pt} $j < \Call{ChunkEnd}{w}; \; j \leftarrow j + 32$} 
    		    \State $\textit{idx23} \leftarrow 0$
				\For{$g=1$ to $L$}
					\For{$b=1$ to $a(D, g)$}
						\State $\textit{idx1} \leftarrow 0$
						\State $\textit{p} \leftarrow 1$
						\If{$threadIdx.x = 0$}
							\State Compute $l$ for which $\text{pos}(l) = b$
						\EndIf
						\For{$t=1$ to $D$}
							\State $div \leftarrow 2^{-l[t]}$
							\State $\textit{idx1} \leftarrow \textit{idx1} \cdot 2^{l[t]} + \lfloor	x[j][t] / div \rfloor$
							\State $left \leftarrow \lfloor x[j][t] / div \rfloor \cdot div$
							\State $right \leftarrow left + div$
							\State $p \leftarrow p \cdot basis(left, right, x[j][t])$
						\EndFor
						\State $r[j] \leftarrow r[j] + \textit{asg1d}[\textit{idx1}	+\textit{idx23}] \cdot p$ 
						\State $\textit{idx23} \leftarrow \textit{idx23} + 2^g$
					\EndFor
				\EndFor
    		\EndFor
    	\EndProcedure
 	\end{algorithmic}
}
\end{algorithm}

We start with optimizations for a better vectorization. For the CPU version,
in \textit{vec1} we use loop unroll-and-jam, loop tiling and we vectorize the
innermost loop $t$. Software pipelining can be done in the unrolled loop, but
$x$ is accessed with stride-$D$ instead of stride-1. Therefore, we change the
layout of $x$ such that $x[j][t] \rightarrow \tilde{x}[(j/\textit{m2}) \cdot D +
t][j \bmod \textit{m2}]$, where $\textit{m2}$ is the size of the tile. Now, a
stride-1 access on $\tilde{x}$ is possible which makes SSE vectorization
efficient. For GPUs, the observation we make is that the loop over dimensions on
line 11 has a non-contiguous access pattern for a warp because at each read
instruction the threads within a warp access elements found at $D$ distance.
\textit{vec1'} improves this by using a technique similar to the one we
described for both \textit{hierarchization} and \textit{evaluation} algorithms
for increasing the throughput through memory coalescing by threads within a
warp. More specific, \textit{vec1'} transposes the matrix $x$ such that instead
of storing the the points horizontally, one point per row, it stores them
vertically in $\tilde{x}[(N/32+1) \cdot D][32]$. Thus, all threads in a warp
access one row in this new matrix for each iteration over dimensions.

Regarding memory locality, \textit{ichg2} does a loop interchange by moving the
loop over interpolation points from line 2 inside the loop over \textit{blocks}
from line 6. In the case of GPUs, we have two variations. The first one,
\textit{ichg2'} corresponds to \textit{ichg2} while the second one,
\textit{ichg2''}, additionally moves the loops over \textit{groups} and
\textit{blocks} outside the kernel. The goal of these optimizations is to
improve read accesses by having the innermost loop as tight as possible over the
block of memory.

The last optimization is a combination of strength reduction and loop invariant
code motion introduced in order to speed up integer operations. \textit{sred1}
precomputes all divisions in lines 12-16 outside the innermost loop and replaces
them with multiplications with the inverse. In addition, $idx1 \leftarrow
idx1 \cdot 2^{l[t]} + \lfloor x[j][t] / div \rfloor$ from line 13 is
replaced with $idx1 \leftarrow idx1 + \lfloor x[j][t] / 2^{l[t]} \rfloor
\cdot 2^{prefix\_sums[t + 1]}$, where $prefix\_sums[p] = \sum_{p=t}^{D-1}l[p]$.
This makes indexing a normal reduction and increases the instruction level
parallelism.

\begin{center}
\begin{table*}[t]
{\small
\hfill{}
\begin{tabular}{|c|l|l|}
  \hline
  \textbf{Category} & \textbf{Abbreviation} & \textbf{Optimizations} \\ 
  \hline
  Memory locality & \textit{ichg1}, \textit{ichg2} & loop interchange \\ 
  \hline
  Vectorization  & \textit{vec1} & loop unroll-and-jam, loop tiling, software pipelining \\ 
  \hline
  \multirow{2}{*}{Integers} & \textit{inv1}, \textit{inv2}, \textit{inv3}, \textit{inv4} & loop invariant code motion \\
  \cline{2-3}
  & \textit{sred1} & strength reduction, loop invariant code motion \\
  \hline
\end{tabular}}
\hfill{}
\caption{Summary of optimizations.}
\label{table:optimizations}
\end{table*}
\end{center}

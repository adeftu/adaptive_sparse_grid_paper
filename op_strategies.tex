\section{Optimization Strategies}
\label{sec:op_strategies}

The computational core of \textit{fastsg} resides in \textit{hierarchize} and
\textit{evaluate} routines. We will not enter here into the details of these
procedures. A summary of them is available in
\cite{Murarasu:2011:CDS:1941553.1941559}.
Instead, we extend the set of loop transformations from \cite{murarasu12fastsg:} and
present them in the context of CPUs and GPUs. We group these optimizations into
3 categories based on their end purpose: memory locality, vectorization, and
operations with integers, synthesized in Table \ref{table:optimizations}.

\subsection{Compression}

\begin{algorithm}[b]
\small{
	\caption{Compression on GPU}
 	\label{alg:hierarchization}                       

 	\begin{algorithmic}[1]
 		\For{$t=1$ to $D$}
 			\For{$g=L$ downto $1$}
 				\State \Call{HierarchizeKernel}{$t, g$}
 			\EndFor
 		\EndFor
 		\algstore{bkbreak}
	\end{algorithmic}
 
	\begin{algorithmic}[1]
  		\algrestore{bkbreak}
  		\Procedure{HierarchizeKernel}{$t, g$}
	 		\For{$j \leftarrow \Call{BlockStart}{g} + lane;$ \\
	 		    \hspace{24pt} $j < \Call{BlockEnd}{g}; \; j\leftarrow j + 32$}
	 			\If{$lane = 0$}
	 				\State $\bar{l} \leftarrow idx2l(j)$
	 			\EndIf
	 			\State $\bar{i} \leftarrow idx2i(j)$
	 			\State $(\bar{ll}, \bar{il}) \leftarrow \text{left}(\bar{l},\bar{i}, t)$
	 			\State $(\bar{lr}, \bar{ir}) \leftarrow \text{right}(\bar{l},\bar{i}, t)$
	 			\State $v1 \leftarrow \textit{asg1d}[\text{agp2idx}(\bar{ll},\bar{il})]$ 
	 			\State $v2 \leftarrow \textit{asg1d}[\text{agp2idx}(\bar{lr},\bar{ir})]$ 
	 			\State $\textit{asg1d}[j] \leftarrow \textit{asg1d}[j] - (v1 + v2) \cdot 0.5$
			\EndFor
		\EndProcedure
	\end{algorithmic}
}
\end{algorithm}

We start with a basic version of \textit{hierarchization} algorithm for GPU
where we loop with $t$ variable over all dimensions, with $g$ variable over all
\textit{groups} and then we call the kernel procedure. Each warp on the GPU
computes the coefficients for one \textit{block} of the sparse grid in order to
achieve a better cache locality and to reduce the thread divergence within
warps. Because the coefficients for one \textit{block} are stored contiguously
in memory, we also want for these values to be accessed simultaneously.
Therefore, in loop $j$, each thread within a warp starts reading at index equal
to its lane and then subsequently every element found at a distance multiple of 32 (the
number of threads within a warp). In this way, a warp will access the memory
contiguously at each read instruction with an increased throughput. In lines
8-9 we use the bijection to move from the global index of the grid point to
$(\bar{l}, \bar{i})$, the \textit{levels} and respectively \textit{index}
vectors representing the coordinates of one point in the grid. We store them in the
shared memory because they are frequently used. The bijection functions use many
integer-only operations. The routines \textit{left} and \textit{right}
from lines 10-11 return the left and respectively right parents (\textit{blocks}
that the current one depends on) of a point in dimension $t$, while
\textit{agp2idx} returns the index where point $(\bar{l}, \bar{i})$ is stored in
the 1d representation of the sparse grid, \textit{asg1d}. The statements in
lines 12-13 exhibit a non-sequential memory access patern.

We now improve this basic algorithm with the goal of exploiting our data
structure and \textit{block}-level decomposition. We start with optimizations
for integer operations. The first observation we make is that computing the
$\bar{l}$ vector for each point in the \textit{block} is redundant because
within a \textit{block}, it has the same value. Therefore loop invariant code
motion optimization \textit{inv1} moves the computation of $\bar{l}$ outside the
$j$ loop.

Next, in \textit{inv2} we can compute for each \textit{block} of norm $g$ the
indices in \textit{asg1d} of all $g-1$ parent \textit{blocks} before the innermost
loop. We store them in an array (in the heap space in the case of CPU
implementation and in the shared memory of a CUDA multiprocessor for the GPU
version). To get the values for these parent points, we can now use this array
to index \textit{asg1d} and get the start of the \textit{block}. What we still need is
the index inside the \textit{block}. This is computed inside the $j$ loop by
converting $\bar{i}$ to the corresponding index.

\textit{inv3} starts from the observation that $(\bar{ll}, \bar{il})$ and
$(\bar{lr}, \bar{ir})$, the \textit{levels} and \textit{index} vectors of the
left and right parents, differ from $\bar{l}$ and $\bar{i}$ of the current point
only in one dimension. Therefore, the conversion mentioned above is not
necessary anymore, thus reducing the complexity from $O(D)$ to $O(1)$.

Now, in the innermost loop over the current \textit{block}, what is left to
compute is the conversion $(\bar{l}(t), \bar{i}(t)) \rightarrow (\bar{ll}(t),
\bar{il}(t))$ and $(\bar{l}(t), \bar{i}(t)) \rightarrow (\bar{lr}(t),
\bar{ir}(t))$ which has a complexity of $O(\bar{l}(t))$.
\textit{inv4} precomputes the results of these conversions before the kernel
launch and stores them in an array. The advantage is that the complexity of the
innermost loop is now $O(1)$, but at a cost of higher memory footprint.

Lastly, in order to improve memory locality and the data access patterns,
\textit{ichg1} optimization does a loop interchange by moving the loop over the
\textit{groups} from line 2 inside the kernel. In this way, one warp will not
only process one \textit{block}, but also all its parents.

\subsection{Decompression}

We move now to the algorithm for \textit{evaluation}. The coordinates of all $N$
points to be evaluated are stored contiguously in a matrix $x[N][D]$ which is
processed in chunks by each warp, each warp computing one chunk of size $w$ in
loop $j$. The memory access pattern for one thread inside of its chunk is the
same as for \textit{hierarchization} algorithm where each thread computes points
stored at indices multiple of 32 starting from the corresponding lane. The $g$
and $b$ loops traverse \textit{groups} and \textit{blocks} respectively.
\textit{idx23} is used to index the beginning of the current \textit{block} in
\textit{asg1d}, while \textit{idx1} idexes the current point in this
\textit{block}. In line 13 we compute the coefficient position, while in line 16
we evaluate the basis function in that point. Line 17 updates the current value
of the coefficient exploiting the \textit{block}-level structure and exhibiting
a non-sequential memory access patern.

\begin{algorithm}[t]
\small{
	\caption{Decompression on GPU}
	\label{alg:evaluation}
	\begin{algorithmic}[1]
 		\Procedure{EvaluateKernel}{$w$}
    		\For{$j \leftarrow \Call{ChunkStart}{w} + lane;$ \\ 
    		\hspace{24pt} $j < \Call{ChunkEnd}{w}; \; j \leftarrow j + 32$} 
    		    \State $\textit{idx23} \leftarrow 0$
				\For{$g=1$ to $L$}
					\For{$b=1$ to $a(D, g)$}
						\State $\textit{idx1} \leftarrow 0$
						\State $\textit{p} \leftarrow 1$
						\If{$threadIdx.x = 0$}
							\State Compute $\bar{l}$ for which $\text{pos}(\bar{l}) = b$
						\EndIf
						\For{$t=1$ to $D$}
							\State $div \leftarrow 2^{-\bar{l}[t]}$
							\State $\textit{idx1} \leftarrow \textit{idx1} \cdot 2^{\bar{l}[t]} + \lfloor	x[j][t] / div \rfloor$
							\State $left \leftarrow \lfloor x[j][t] / div \rfloor \cdot div$
							\State $right \leftarrow left + div$
							\State $p \leftarrow p \cdot basis(left, right, x[j][t])$
						\EndFor
						\State $r[j] \leftarrow r[j] + \textit{asg1d}[\textit{idx1}	+\textit{idx23}] \cdot p$ 
						\State $\textit{idx23} \leftarrow \textit{idx23} + 2^g$
					\EndFor
				\EndFor
    		\EndFor
    	\EndProcedure
 	\end{algorithmic}
}
\end{algorithm}

We start with optimizations for a better vectorization. For the CPU version,
in \textit{vec1} we use loop unroll-and-jam, loop tiling and we vectorize the
innermost loop $t$. Software pipelining can be done in the unrolled loop, but
$x$ is accessed with stride-$D$ instead of stride-1. Therefore, we change the
layout of $x$ such that $x[j][t] \rightarrow \tilde{x}[(j/\textit{m2}) \cdot D +
t][j \bmod \textit{m2}]$, where $\textit{m2}$ is the size of the tile. Now, a
stride-1 access on $\tilde{x}$ is possible which makes SSE vectorization
efficient. For GPUs, the observation we make is that the loop over dimensions on
line 11 has a non-contiguous access pattern for a warp because at each read
instruction the threads within a warp access elements found at $D$ distance.
\textit{vec1'} improves this by using a technique similar to the one we
described for both \textit{hierarchization} and \textit{evaluation} algorithms
for increasing the throughput through memory coalescing by threads within a
warp. More specific, \textit{vec1'} transposes the matrix $x$ such that instead
of storing the the points horizontally, one point per row, it stores them
vertically in $\tilde{x}[(N/32+1) \cdot D][32]$. Thus, all threads in a warp
access one row in this new matrix for each iteration over dimensions.

Regarding memory locality, \textit{ichg2} does a loop interchange by moving the
loop over interpolation points from line 2 inside the loop over \textit{blocks}
from line 6. In the case of GPUs, we have two variations. The first one,
\textit{ichg2'} corresponds to \textit{ichg2} while the second one,
\textit{ichg2''}, additionally moves the loops over \textit{groups} and
\textit{blocks} outside the kernel. The goal of these optimizations is to
improve read accesses by having the innermost loop as tight as possible over the
block of memory.

The last optimization is a combination of strength reduction and loop invariant
code motion introduced in order to speed up arithmetic operations.
\textit{sred1} precomputes all divisions in lines 12-16 outside the innermost
loop and replaces them with multiplications with the inverse. In addition, $idx1
\leftarrow idx1 \cdot 2^{\bar{l}[t]} + \lfloor x[j][t] / div \rfloor$ from line
13 is replaced with $idx1 \leftarrow idx1 + \lfloor x[j][t] / 2^{\bar{l}[t]}
\rfloor \cdot 2^{\textit{prefix\_sums}[t + 1]}$, where $\textit{prefix\_sums}[p]
= \sum_{p=t}^{D-1}\bar{l}[p]$. This makes indexing a normal reduction and
increases the instruction level parallelism.

\begin{center}
\begin{table*}[t]
{\small
\hfill{}
\begin{tabular}{|c|l|l|}
  \hline
  \textbf{Category} & \textbf{Abbreviation} & \textbf{Optimizations} \\ 
  \hline
  Memory locality & \textit{ichg1}, \textit{ichg2} & loop interchange \\ 
  \hline
  Vectorization  & \textit{vec1} & loop unroll-and-jam, loop tiling, software pipelining \\ 
  \hline
  \multirow{2}{*}{Integers} & \textit{inv1}, \textit{inv2}, \textit{inv3}, \textit{inv4} & loop invariant code motion \\
  \cline{2-3}
  & \textit{sred1} & strength reduction, loop invariant code motion \\
  \hline
\end{tabular}}
\hfill{}
\caption{Summary of optimizations.}
\label{table:optimizations}
\end{table*}
\end{center}
\section{Introduction}
The number of processor architectures has grown significantly in the recent
years. We saw the appearance of accelerators such a GPUs as a response to the
many obstacles met by processor architects, e.g. the frequency and memory walls.
GPUs are application specific architectures, well suited for data parallelism,
for code with regular memory access patterns such as those available in
numerical or graphics applications. Compared to CPUs, GPUs contain simpler cores
but compensate with their higher number (tens of cores compared to 4 - 8 for
CPUs). The cores operate at a lower frequency (often 1 GHz against 2 - 3 GHz for
CPUs) and are in-order, hiding the latency of the instruction pipeline by
interleaving on each core the execution of thousands of threads.

A simple abstraction for GPUs is to consider them many-core architectures, with
a large number of cores and with each core including a large vector units
allowing to compute up to 32 flops per cycle (64 flops for FMAD). This view of
GPUs is not immediately visible in the programming manuals released by GPU
vendors. CUDA is an option for programming Nvidia GPUs. The vector units are not
directly visible in CUDA. Indirectly, the SIMD nature of GPUs reveals itself
when applying optimizations for bank conflicts and memory access coalescing (see
CUDA manual for more details). Seeing the GPUs as vector processors is at the
foundation of Volkov's paper from SC 2008 on optimizing matrix multiplication.
His tuning strategy was afterwards included in the cublas library from Nvidia.

% CUDA can be misleading when referring to the architecture. Of course, a
% trade-off should be clarified at this point. It does what it does to simplify
% programming with the cost of hiding the real hardware. Through CUDA lenses, the
% GPU is an array of hundreds of scalar cores which for us they generally map to
% SIMD lanes. The cores are at the immediate upper level contained in a Streaming
% Multi-processor (SM). For Nvidia Fermi GPUs, an SM contains 32 of such cores. SM
% has low latency, local memory which is reconfigurable allowing for different
% ratios between the scratchpad and the L1 cache. At a upper layer we have an 768
% KB L2 cache improving the access to the RAM on the GPU. This is the hardware
% part. On top of it, CUDA defines how the software maps to it. The main concept
% here is the thread. The threads are implicitly executed as warps of 32, each
% branch taken by one of the 32 reduces performance, much as it happens when
% masking the lanes of a SIMD unit. CUDA threads are grouped explicitly this time
% in blocks and a block is executed on an SM. Communication among threads is
% allowed only within a block.

When reasoning about code transformations that improve performance on a wide
range of architectures, we do not want to lose ourselves in the details of the
programming models. We need the real view of the hardware not a simplified view.
Ideally, what we want is to develop high-level code transformations for which it
can be decided at design time performance improvement they give on a given
architecture, i.e. CPUs or GPUs. We do not expect that the sequence of
transformations remains the same. To simplify the porting of applications from
one architecture to another, it is essential to determine the common / different
points of the architectures and map the optimizations accordingly. As an
example, CPUs and GPUs have vector units and AOS to SOA is a transformation that
often is required by both. Given the converge of CPUs and GPUs seen in CPUs that
have doubled the width of the SIMD units (Intel Sandy Bridge has 2x the bits of
Intel Nehalem, 128 bits) and GPUs that received 2 level, coherent caches, we
expect that more transformations, e.g. for locality, to be shared between codes
for CPUs and GPUs.

To test these concepts, we consider a computational steering application. The
main goal in this application is to allow for a smooth, real-time interaction
with compressed simulation data. We have three phases (or algorithms) being at the
core of the compression and decompression of the simulation data. These
algorithms are in fact based on a numerical method called the sparse grid
technique which allows for a hierarchical and efficient representation of
high-dimensional functions. Using it, it was shown that sheer amounts of
simulation data can be managed efficiently both with regard to time and space,
i.e. memory footprint.

For enhanced performance, it was important to benefit from the latest
developments in hardware. But starting from scratch with every hardware release
is far from being a productive approach. Instead, what we followed was to
develop optimizations for loops and data layout at the same level at which we
develop algorithms. We realized that many of the transformations we are applying
are valid for both CPUs and GPUs, considering the above view of GPUs as a vector
processors. The main contributions of our paper are as follows:

\begin{itemize}
  \item We port dimensionally adaptive sparse grids algorithms on GPUs and
  report a speedup of ??? compared to the CPU versions.
  \item We present a study on the performance of a subset of common loop
  transformations for two different architectures, CPUs and GPUs. We describe
  the challenges met along the way and their solutions.
  \item We report speedup results that, for the considered computational
  steering application, accelerate the state-of-the art implementation up to a
  factor of 6.2x.
\end{itemize}



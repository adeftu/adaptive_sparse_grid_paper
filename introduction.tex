\section{Introduction}
The number of processor architectures has grown significantly in the recent
years. We saw the appearance of accelerators such a GPUs as a response to the
many obstacles met by processor architects, e.g. the frequency and memory walls.
GPUs are application specific architectures, well suited for data parallelism,
for code with regular memory access patterns such as those available in
numerical or graphics applications. Compared to CPUs, GPUs contain simpler cores
but compensate with their higher number (tens of cores compared to 4 - 8 for
CPUs). The cores operate at a lower frequency (often 1 GHz against 2 - 3 GHz for
CPUs) and are in-order, hiding the latency of the instruction pipeline by
interleaving on each core the execution of thousands of threads.

A simple abstraction for GPUs is to consider them many-core architectures, with
a large number of cores and with each core including large vector units
allowing to compute up to 32 single precision flops per cycle (64 flops for
FMAD). Compute Unified Device Architecture (CUDA) is an option for programming
Nvidia GPUs. The vector units are not directly visible in CUDA. The SIMD nature
of GPUs reveals itself when applying various optimizations, e.g. for bank
conflicts and memory access coalescing \cite{cuda}. This view of GPUs as vector
processors is at the foundation of \cite{Volkov:2008:BGT:1413370.1413402}.

When reasoning about code transformations that improve performance on a wide
range of architectures, we do not want to lose ourselves in the details of the
programming models. A high-level simplified view allows us to have more
flexibility when adapting the code to new architectures. Considering this
trade-off, our goal is therefore to develop high-level code transformations that
address common features among different devices. We do not expect that the
sequence of transformations remains the same. To simplify the porting of
applications from one architecture to another, it is essential to determine the
common and different aspects of the architectures and map the optimizations
accordingly. As an example, CPUs and GPUs have vector units and
array-of-structures to structure-of-arrays is a transformation that often is
required by both. Given the convergence of CPUs and GPUs seen in CPUs that have
doubled the width of the SIMD units and GPUs that received 2 level coherent
caches \cite{fermi}, we expect that more transformations, e.g. for locality, to
be shared between codes for CPUs and GPUs.

To test these concepts, we consider a computational steering application, the
main goal of which is to allow for a smooth, real-time interaction with
compressed simulation data. We have two phases (or algorithms) being at the core
of the compression and decompression of the simulation data. These algorithms
are in fact based on a numerical method called the sparse grid technique which
allows for a hierarchical and efficient representation of high-dimensional
functions, similar to wavelet series. Using it, it was shown that sheer amounts
of simulation data can be managed efficiently \cite{Butnaru201156}.

For enhanced performance, it was important to benefit from the latest
developments in hardware. But starting from scratch with every hardware release
is far from being a productive approach. Instead, what we followed was to
develop optimizations for loops and data layout at the same level at which we
develop algorithms. We realized that many of the transformations we are applying
are valid for both CPUs and GPUs, considering the above mentioned view of GPUs
as vector processors. Moreover, we consider that our study is important giving
that more and more programming models are emerging and the open question is if
we can design optimizations which are portable across processor architectures.
The main contributions of our paper are as follows:

\begin{itemize}
  \item We port dimensionally truncated sparse grids algorithms on GPUs and
  report a speedup of 28.3x compared to the CPU versions.
  \item We present a study on the performance of a subset of common loop
  transformations for two different architectures, CPUs and GPUs. We describe
  the challenges met along the way and their solutions.
  \item We report speedup results that, for the considered computational
  steering application, accelerate the state-of-the-art implementation up to a
  factor of 6.2x for compresion and 1.5x for decompression.
\end{itemize}

The rest of the paper is structured as follows. In
Section~\ref{sec:related_work} we place our work in the field of application
optimizations and sparse grids and relate to previous published papers.
Section~\ref{sec:cpus_vs_gpus} highlights the similarity between CPUs and GPUs,
seen as modern processors which borrow architectural features from each other,
allowing for common optimizations to be used.
In Section~\ref{sec:optimizations} we describe a set of loop transformation
suitable for both architectures in this context. Section~\ref{sec:comp_steering}
introduces in the field of sparse grids and presents the core functionality of
our library. Section~\ref{sec:op_strategies} shows how we can apply
optimizations for both CPUs and GPUs based on the aforementioned ideas. In
Section~\ref{sec:evaluation} we present our results and show that despite using
different architectures, similarities in hardware characteristics can make
optimizing code easier. Finally, Section~\ref{sec:conclusion} concludes our
work.

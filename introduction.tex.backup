\section{Introduction}
The number of processor architectures has grown significantly in the recent years. We saw the appearance of accelerators such a GPUs as responses to the many obstacles hit by the processor architects, namely the frequency wall and the memory wall. GPUs are application specific architectures, extremely well suited for data parallelism, for codes with regular access patterns such as those available in numerical or graphics applications. Compared to CPUs, GPUs have simpler cores but in a higher number (tens of cores compared to 4 - 8 for CPUs), operating at a lower frequency (often 1 GHz against 2 - 3 GHz for CPUs). Further, GPUs cores are in-order, hiding the lantency of the instruction pipeline by interleaving on each core the execution of thousands of threads.

A simple abstraction for GPUs is to consider them many-core architectures, with a large number of cores and with each core including a large vector units allowing to compute up to 32 flops per cycle (64 flops for FMAD). This view of GPUs is not immediately accesible after reading the programming manual released by GPU vendors. CUDA is one option for programming Nvidia GPUs. The vector units are not directly visible in CUDA. Indirectly, the SIMD nature of GPUs reveals itself when applying optimizations for bank conflicts and memory access coalescing (see CUDA manual for more details). Seeing the GPUs as vector processors is at the foundation of Volkov's paper from SC 2008 on optimizing matrix multiplication. His tuning strategy was afterwards included in the cublas library from Nvidia.

CUDA can be misleading when refering to the architecture. Of course, a tradeoff should be clarified at this point. It does what it does to simplify programming with the cost of hiding the real hardware. Through CUDA lenses, the GPU is an array of hundreds of scalar cores which for us they generally map to SIMD lanes. The cores are at the immediate upper level contained in a Streaming Multi-processor (SM). For Nvidia Fermi GPUs, an SM contains 32 of such cores. SM has low latency, local memory which is reconfigurable allowing for different ratios between the scratchpad and the L1 cache. At a upper layer we have an 768 KB L2 cache improving the access to the RAM on the GPU. This is the hardware part. On top of it, CUDA defines how the software maps to it. The main concept here is the thread. The threads are implicitly executed as warps of 32, each branch taken by one of the 32 reduces performance, much as it happens when masking the lanes of a SIMD unit. CUDA threads are grouped explicitly this time in blocks and a block is executed on an SM. Communication among threads is allowed only within a block.

When reasoning about code transformations that improve performance on a wide range of architectures, we do not want to lose ourselves in the details of the programming models. We need the real view of the hardware not a simplified view. Ideally, what we want is to develop high-level code transformations for which it can be decided at design time performance improvement they give on a given architecture, i.e. CPUs or GPUs. We do not expect that the sequence of transformations remains the same. To simplify the porting of applications from one architecture to another, it is essential to determine the common / different points of the architectures and map the optimizations accordingly. As an example, CPUs and GPUs have vector units and AOS to SOA is a transformation that often is required by both. Given the converge of CPUs and GPUs seen in CPUs that have doubled the width of the SIMD units (Intel Sandy Bridge has 2x the bits of Intel Nehalem, 128 bits) and GPUs that received 2 level, coherent caches, we expect that more transformations, e.g. for locality, to be shared between codes for CPUs and GPUs.

To test these concepts, we consider a computational steering application. The main goal in this application is allow for a smooth, real-time interaction with compressed simulation data. We have three algorithms for compression and decompression of simulation data. These algorithms are based on a numerical method called the sparse grid technique which allows us to manage efficiency sheer amounts of simulation data.

